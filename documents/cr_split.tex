\documentclass[12pt]{article}

\newcommand{\documentname}{\textsl{note}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\LSST}{\project{LSST}}
\newcounter{hogg}
\newenvironment{hogglist}{\setcounter{hogg}{0}}{}
\newcommand{\hoggitem}{\refstepcounter{hogg}\textsl{(\thehogg)}}

\begin{document}

\section{Introduction}

The default plan of the \LSST\ project is to split each 30-s exposure into two
15-s sub-exposures, primarily to permit identification of cosmic rays (CR).
It now appears that the CR justification is weaker, and other considerations
might dominate.
Here we ask the following ill-posed question:
Given an overall exposure constraint of 30~s (plus the time to take two detector
reads), what is the best split into sub-exposures?
If there is
\begin{hogglist}
  \hoggitem~finite read noise,
  \hoggitem~no concerns about detector saturation,
  \hoggitem~no issue with identifying CRs,
  \hoggitem~no variation in the sky foreground intensity, atmospheric
  transparency, astrometric distortions (WCS), or point-spread function (PSF),
  and
  \hoggitem~no variation in the fluxes or positions of sources of interest,
\end{hogglist}
then the answer is simple:
Take a single exposure for 30~s, plus the extra exposure time you gain by not
performing the second detector read.
This will deliver maximum signal-to-noise on the (by assumption) completely
fixed scene.

While there is definitely finite read noise in the \LSST\ camera, the other
conditions---no issues with saturation or CR identification and no variation in
the atmosphere or sources---do not hold in general.
In particular, the PSF is expected to vary rapidly on short time-scales.
As we consider the issues of understanding saturated sources, removing CRs,
accounting for atmospheric variability, and measuring variable or moving
sources, we are likely to find that different exposure-time choices affect
different goals differently.
This is what makes the general question ill-posed; we seek some well-posed
questions that are related, or answer the ill-posed question in useful limiting
cases.

For the purposes of this \documentname, we will presume that the
\LSST\ hardware, filters, and site are all now fixed in design; this makes it
unproductive to consider non-\LSST\ levels or types of read noise, saturation
effects, character of CR events, and mean and variability in atmospheric
properties.
We will also presume that there is a very good, very realistic \LSST\ data
simulator, that can simulate all the effects of the electronics, optics,
atmosphere, radiation environment, and astronomical sources.
Finally, we will consider the 30-s overall exposure-time window to be a fixed
requirement.
In practice, this may also be up for discussion, but that is beyond our current
scope.

With these ``rules of the game,'' the only parameter we will consider varying in
this \documentname\ is the \emph{split time} $t_s$, or the duration of the
shorter of the two exposures into which the 30-s exposure is split.
The split time $t_s$ can be varied in the range $0\leq t_s<15$~s.
The value $t_s=0$ has a special character, because it permits a slightly longer
total exposure time (as it saves the project a full read) and it reduces the
data rate by a factor of two; in what follows we won't consider these advantages
to $t_s=0$, but they could be added in to a more complete analysis.
The open questions are:
\begin{hogglist}
  \hoggitem~What information is gained or lost about saturated (bright) sources
  as the split time $t_s$ is varied?
  \hoggitem~How does the capture of variations in the atmosphere improve or hurt\ldots HOGG!
  \hoggitem~How do the accuracy of measurements of moving objects\ldots HOGG!
  \hoggitem~Is there any important stellar variablity that\ldots HOGG!
\end{hogglist}

Just to whet the appetite, let's think a bit about the simplest problem:
How does the split time affect our ability to measure the positions
and brightnesses of non-moving, non-varying stars?
At first it might seem that it can't possibly matter.
But the fact that the WCS and PSF vary between the exposures suggests
that the two exposures taken together might provide more information
than their simple sum or average or co-addition.
For example, consider the point that a very short exposure might often
or occasionally deliver a very spiky or compact PSF.
This exposure will have low signal-to-noise but might deliver high
angular resolution for brighter sources.

We are going to answer the questions we care about in the context of
the \LSST\ system, using the \LSST\ image simulation infrastructure.
This helps with keeping the problems well-posed.
That said, in the end, the results we get will depend only on a few fundamental
controlling parameters, such as the read noise, the sky brightness, the PSF
variability (in appropriate units), and the angular densities of certain kinds
of sources.
For this reason, all of the machinery used in this study is generalizable to
other hardware and other scientific goals.
We will try to bring out the relevant scalings in each part of what follows.
That is, we will try to describe (at least roughly) how the results would adjust
in response to changes in system properties.

\section{Measurement generalities}

In comparing different choices of split time $t_s$, we will have to make
quantitative comparisons between measurements.
We will presume that the \LSST\ pipelines deliver---for point sources at
least---the best measurements possible, given the noise, WCS, and PSF.
What are the best measurements possible for the brightness and position
of a star, given finite read and sky noise and a non-trivial PSF?
The answer is given by the Cram\'er--Rao Bound, which is related to
derivatives of the likelihood function.
Yada yada\ldots HOGG!

This is all na\"ive of course:
The Cram\'er--Rao Bound only applies when the likelihood function is
\emph{correct}; that is, it only applies when the data are generated by
the statistical process described by the likelihood function.
This can't be true for many reasons, not the least of which is that,
in any finite eposure, there is no location on the focal plane at which
the pipelines can know the PSF precisely.
All knowledge of the PSF is statistical or probabilistic.
What to do about this?\ldots HOGG!

...What changes for saturated objects, and objects that are motion-blurred?

\section{System properties and simulations}

...We rely completely on \ImSim\ (CITE Peterson).

...We run \ImSim\ in its most painful mode.

...The fundamental assumptions of \ImSim\ are\ldots

...The biggest weaknesses of \ImSim\ are\ldots HOGG!

\section{Saturation and bright sources}

Much of what is written in this section depends on how the gains on the
camera are set.

\section{Static, non-variable stars}

Even in the ultra-simplified case in which the sky contains a static
pattern of non-moving, non-variable stars, there is no trivial result for
the split time $t_s$\ldots HOGG!

...Compute how PSF estimation ought to improve with more exposures with
more PSF diversity.

...Note that this has an impact for the weak lensing goals.

\section{Fast-moving objects}

\section{Variable stars}

\section{Discussion}

\end{document}
